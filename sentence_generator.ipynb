{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2uFrD6L--jl"
      },
      "source": [
        "class gen_sentences:\r\n",
        "  def __init__(self):\r\n",
        "    # import spacy\r\n",
        "    import requests as r\r\n",
        "    from bs4 import BeautifulSoup as bs_4\r\n",
        "    #instalar el modulo \"es_core_news_sm\" en caso de que no este instalado\r\n",
        "    try:\r\n",
        "      import es_core_news_sm\r\n",
        "    except ModuleNotFoundError:\r\n",
        "      !python -m spacy download es_core_news_sm\r\n",
        "      import es_core_news_sm\r\n",
        "    \r\n",
        "    self.BeautifulSoup = bs_4\r\n",
        "    self.requests = r\r\n",
        "    self.nlp = es_core_news_sm.load()\r\n",
        "  \r\n",
        "  def get_ner(self,oracion=None,Lem=False):\r\n",
        "    word_dict = {}\r\n",
        "    word_ = []\r\n",
        "    class_ = []\r\n",
        "    doc = self.nlp(oracion)\r\n",
        "\r\n",
        "    strg_lemma = \"\"\r\n",
        "    if Lem == True:\r\n",
        "      for token in doc:\r\n",
        "        strg_lemma = strg_lemma + token.lemma_ + \" \"\r\n",
        "    else:\r\n",
        "      strg_lemma = str(doc)\r\n",
        "    \r\n",
        "    doc = self.nlp(strg_lemma)\r\n",
        "    for token in doc:\r\n",
        "      word_.append(token.text)\r\n",
        "      class_.append(token.pos_)\r\n",
        "    word_dict.update({\"word\":word_,\"class\":class_})\r\n",
        "    return word_dict\r\n",
        "  \r\n",
        "  def get_words(self,oracion=None,Lem=False):\r\n",
        "    doc = self.nlp(oracion)\r\n",
        "    strg_lemma = \"\"\r\n",
        "    if Lem == True:\r\n",
        "      #Lematizamos la oración, es decir convertir las palabras a su forma base común\r\n",
        "      for token in doc:\r\n",
        "        strg_lemma = strg_lemma + token.lemma_ + \" \"\r\n",
        "    else:\r\n",
        "      strg_lemma = str(doc)\r\n",
        "    #Identificar \"NOUNS\" \"ADJ\" \"ADVERBS\" y \"VERBS\"\r\n",
        "    doc_lemma = self.nlp(strg_lemma)\r\n",
        "    words = []\r\n",
        "    word_class = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\r\n",
        "    for token in doc_lemma:\r\n",
        "      for i in word_class:\r\n",
        "        if token.pos_ == i:\r\n",
        "          words.append(token.text)\r\n",
        "    return words\r\n",
        "\r\n",
        "  def paraphrase(self,oracion=None,Lem=False):\r\n",
        "    doc = self.nlp(oracion)\r\n",
        "    final_sentences=[oracion.lower()]\r\n",
        "\r\n",
        "    strg_lemma = \"\"\r\n",
        "    if Lem == True:\r\n",
        "      for token in doc:\r\n",
        "        strg_lemma = strg_lemma + token.lemma_ + \" \"\r\n",
        "    else:\r\n",
        "      strg_lemma = str(doc)\r\n",
        "    \r\n",
        "    doc_lemma = self.nlp(strg_lemma)\r\n",
        "    \r\n",
        "    words = []\r\n",
        "    word_class = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\",]\r\n",
        "    for token in doc_lemma:\r\n",
        "      for i in word_class:\r\n",
        "        if token.pos_ == i:\r\n",
        "          words.append(token.text)\r\n",
        "\r\n",
        "    \r\n",
        "    for it_f,word in enumerate(words):\r\n",
        "      sentences=[]\r\n",
        "      url = 'http://www.wordreference.com/sinonimos/'\r\n",
        "      enlace = word\r\n",
        "      buscar = url + enlace\r\n",
        "      resp = self.requests.get(buscar)\r\n",
        "      bs = self.BeautifulSoup(resp.text,'html')\r\n",
        "      lista = bs.find_all(class_ = 'trans clickable')\r\n",
        "\r\n",
        "      \r\n",
        "      for response in lista:\r\n",
        "        syn = response.find('li')\r\n",
        "\r\n",
        "      syn_array = []\r\n",
        "      for T in syn:\r\n",
        "        array = T.split()\r\n",
        "        for it, value in enumerate(array):\r\n",
        "          syn_array.append(value.replace(\",\",\"\"))\r\n",
        "\r\n",
        "    \r\n",
        "      if it_f == 0:\r\n",
        "        for i_, iter in enumerate(syn_array):\r\n",
        "          final_sentences.append(strg_lemma.replace(word,iter).lower())\r\n",
        "      if it_f > 0:\r\n",
        "        for sentence in final_sentences:\r\n",
        "          for iter_ in syn_array:\r\n",
        "            sentences.append(sentence.replace(word,iter_).lower()) \r\n",
        "        final_sentences = final_sentences + sentences\r\n",
        "        \r\n",
        "    return final_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWOQ9Gi1D7nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e09e25-2efc-44d8-98b7-7aaa71ffd1cc"
      },
      "source": [
        "ner_model=gen_sentences() #Instanciamos la clase\r\n",
        "#Solo descarga una vez el modelo \"es_core_news_sm\", si lo vuelves a instanciar debido a alguna modificación\r\n",
        "#en el codigo, este no lo descargará denuevo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp36-none-any.whl size=16172935 sha256=0423716badfdb0a7901fd93cc363bcc58a80b9ed4569d19199e524bc41a393fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-som9ewln/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWd-g866TShF",
        "outputId": "7b96e1dd-526e-4bd1-9a3c-5728207faaa1"
      },
      "source": [
        "#Medicion del tiempo de respuesta, esto con motivos de optimización de las funciones de la clase mas adelante\r\n",
        "from time import time\r\n",
        "tiempo_inicial = time()\r\n",
        "ner_model.get_ner('hola savvy A, ¿Podrías mostrar la definición de x_1 maravillado 8 podrido lentamente ?')\r\n",
        "tiempo_final = time()\r\n",
        "tiempo_ejecucion = tiempo_final - tiempo_inicial\r\n",
        "print(tiempo_ejecucion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01903700828552246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EuVMYe-qFG_",
        "outputId": "97a31891-9634-442e-e636-bc5ee9665896"
      },
      "source": [
        "#Obtenemos la clase a la que pertenece cada palabra \"VERB,NOUN,ADJ,ADV,PUNCT,PROPN,NUM,etc.\"\r\n",
        "NER_=ner_model.get_ner('Hola Savvy, ¿Podrias mostrar la definición de x_1?')\r\n",
        "for it,i in enumerate(NER_[\"word\"]):\r\n",
        "  print(\"Palabra: \",i,\" CLASE: >>\",NER_[\"class\"][it],\"<<\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Palabra:  Hola  CLASE: >> PROPN <<\n",
            "Palabra:  Savvy  CLASE: >> PROPN <<\n",
            "Palabra:  ,  CLASE: >> PUNCT <<\n",
            "Palabra:  ¿  CLASE: >> PUNCT <<\n",
            "Palabra:  Podrias  CLASE: >> PROPN <<\n",
            "Palabra:  mostrar  CLASE: >> VERB <<\n",
            "Palabra:  la  CLASE: >> DET <<\n",
            "Palabra:  definición  CLASE: >> NOUN <<\n",
            "Palabra:  de  CLASE: >> ADP <<\n",
            "Palabra:  x_1  CLASE: >> PROPN <<\n",
            "Palabra:  ?  CLASE: >> PUNCT <<\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqaoIFrGRT2q",
        "outputId": "172aeec8-d66a-4770-e564-ee8a695c387e"
      },
      "source": [
        "#Obtener las palabras que pueden ser utilizadas para desarrollar mas oraciones\r\n",
        "#Obtener los \"VERBS\", \"NOUNS\", \"ADJ\" y \"ADV\"\r\n",
        "ner_model.get_words('hola savvy a, ¿Podrias mostrar la definición de x_1?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mostrar', 'definición']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7j6XwjF8VBS",
        "outputId": "3e672703-88d7-4b02-90e7-04a010d6c3be"
      },
      "source": [
        "#Esta funcion genera directamente nuevas oraciones a partir de los sinonimos que existan de cualquiera de las palabras\r\n",
        "#dentro de la oracion\r\n",
        "#ner_model.paraphrase(\"mostrar la definición de Capital Financiero\")\r\n",
        "ner_model.paraphrase(\"¿cuál es el motivo de ahorrar?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['¿cuál es el motivo de ahorrar?',\n",
              " '¿cuál es el originar de ahorrar?',\n",
              " '¿cuál es el causar de ahorrar?',\n",
              " '¿cuál es el promover de ahorrar?',\n",
              " '¿cuál es el producir de ahorrar?',\n",
              " '¿cuál es el motivo de economizar?',\n",
              " '¿cuál es el motivo de guardar?',\n",
              " '¿cuál es el motivo de atesorar?',\n",
              " '¿cuál es el motivo de conservar?',\n",
              " '¿cuál es el motivo de reservar?',\n",
              " '¿cuál es el originar de economizar?',\n",
              " '¿cuál es el originar de guardar?',\n",
              " '¿cuál es el originar de atesorar?',\n",
              " '¿cuál es el originar de conservar?',\n",
              " '¿cuál es el originar de reservar?',\n",
              " '¿cuál es el causar de economizar?',\n",
              " '¿cuál es el causar de guardar?',\n",
              " '¿cuál es el causar de atesorar?',\n",
              " '¿cuál es el causar de conservar?',\n",
              " '¿cuál es el causar de reservar?',\n",
              " '¿cuál es el promover de economizar?',\n",
              " '¿cuál es el promover de guardar?',\n",
              " '¿cuál es el promover de atesorar?',\n",
              " '¿cuál es el promover de conservar?',\n",
              " '¿cuál es el promover de reservar?',\n",
              " '¿cuál es el producir de economizar?',\n",
              " '¿cuál es el producir de guardar?',\n",
              " '¿cuál es el producir de atesorar?',\n",
              " '¿cuál es el producir de conservar?',\n",
              " '¿cuál es el producir de reservar?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InOdpEoC370q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}